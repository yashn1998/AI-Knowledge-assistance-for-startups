Nice — I cleaned up your Level-1 preprocessor and upgraded it to a Level-2 version that:

Writes output to CSV (configurable path).

Stops accidentally dropping numeric tokens (keeps numbers by default; optional to remove).

Uses spaCy (advanced model) for NER and lemmatization (configurable, with graceful fallback).

Removes stopwords (spaCy stoplist) and accepts custom stopwords.

Preserves case / serial IDs (configurable).

Adds extra_removals regex hooks so you can remove lines/phrases from your screenshot (e.g. “Ln 9, Col 54”, UI footers, “this is Date should be removed…”).

Keeps and extends the original masking (emails, phones, URLs, IPs, SSNs, CC-like sequences, addresses).

Returns an audit log of original → cleaned values per column and can save CSV.


Below is the full ready-to-run script. Read the short usage notes that follow it (including spaCy install suggestions).

"""
Level-2 Text Preprocessor
- Saves processed DataFrame to CSV.
- Keeps numbers by default (no accidental removal).
- Uses spaCy for NER + lemmatization (configurable). Prefers advanced model if available.
- Stopword removal using spaCy's stopwords; accepts custom stopwords.
- Extra regex removal hooks for image notes / UI artifacts.
"""

import re
import unicodedata
from typing import List, Tuple, Dict, Optional, Pattern, Any
import pandas as pd

# Optional spaCy
_SPACY_AVAILABLE = False
try:
    import spacy
    _SPACY_AVAILABLE = True
except Exception:
    _SPACY_AVAILABLE = False

class TextPreprocessorV2:
    def __init__(self,
                 spacy_model: str = "en_core_web_trf",
                 prefer_spacy_model_order: List[str] = None):
        """
        spacy_model: preferred model name (advanced by default). If not installed,
                     class will attempt fallbacks in prefer_spacy_model_order then raise with instructions.
        """
        self.nlp = None
        self.spacy_model = spacy_model
        self._load_attempted = False

        # default fallbacks
        if prefer_spacy_model_order is None:
            self.prefer_spacy_model_order = [
                spacy_model,
                "en_core_web_lg",
                "en_core_web_md",
                "en_core_web_sm"
            ]
        else:
            self.prefer_spacy_model_order = prefer_spacy_model_order

        # Regex patterns
        self.re_url = re.compile(r"""(?i)\b((?:https?://|www\.)\S+)\b""")
        self.re_email = re.compile(r"""([a-zA-Z0-9._%+\-]+@[a-zA-Z0-9.\-]+\.[a-zA-Z]{2,})""")
        self.re_phone = re.compile(r"""(\+?\d{1,3}[-.\s]?)?(\(?\d{2,4}\)?[-.\s]?)?[\d\-.\s]{6,15}\d""")
        self.re_ip = re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b')
        self.re_credit = re.compile(r'\b(?:\d[ -]*?){13,19}\b')
        self.re_ssn = re.compile(r'\b\d{3}-\d{2}-\d{4}\b')
        self.re_datetime = re.compile(
            r'\b(?:\d{1,2}[:/-]\d{1,2}[:/-]\d{2,4}|\d{4}[-/]\d{2}[-/]\d{2}|\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s+\d{1,2},?\s+\d{4})\b',
            re.IGNORECASE)
        self.re_time = re.compile(r'\b\d{1,2}:\d{2}(?::\d{2})?(?:\s?[APMapm]{2})?\b')
        self.re_url2 = re.compile(r'\bhttps?://\S+|\bwww\.\S+\b', re.IGNORECASE)
        self.re_zip = re.compile(r'\b\d{5}(?:-\d{4})?\b')
        self.re_case_id = re.compile(r'\b[A-Z]{2,}\d{0,3}(?:-[A-Z0-9]+){2,}\b')
        self.re_punct = re.compile(r'[^\w\s-]')  # keeps digits and underscores
        self.re_nonprint = re.compile(r'[\x00-\x1f\x7f-\x9f]')
        self.re_punctseq = re.compile(r'^[\W_]+$')

        # Additional default UI / screenshot artifacts to remove
        self.default_extra_removals = [
            re.compile(r'Ln\s*\d+,\s*Col\s*\d+', re.IGNORECASE),
            re.compile(r'Windows\s*\(CRLF\)', re.IGNORECASE),
            re.compile(r'\b(?:AM|PM|am|pm)\b', re.IGNORECASE),  # times at line end
            re.compile(r'\b\d{1,2}/\d{1,2}/\d{2,4}\b'),  # short dates like 9/19/2025
            re.compile(r'this is Date should be removed[^\n]*', re.IGNORECASE),
            re.compile(r'we have to clean dash and maskings from this', re.IGNORECASE),
            # Add other screenshot-specific patterns here if needed
        ]

        # placeholders
        self._mask_tokens = {
            'email': '<EMAIL>',
            'phone': '<PHONE>',
            'url': '<URL>',
            'ip': '<IP>',
            'date': '<DATE>',
            'time': '<TIME>',
            'credit': '<CREDIT>',
            'ssn': '<SSN>',
            'address': '<ADDRESS>',
            'case_id': '<CASE_ID>',
            'person': '<PERSON>',
            'org': '<ORG>',
            'location': '<LOCATION>',
        }

    def _ensure_spacy(self):
        if self.nlp is not None:
            return
        if not _SPACY_AVAILABLE:
            raise RuntimeError("spaCy not installed. Install spaCy and a model: pip install spacy ; python -m spacy download en_core_web_trf (or en_core_web_lg / en_core_web_sm).")
        # try preferred models in order
        last_err = None
        for model in self.prefer_spacy_model_order:
            try:
                self.nlp = spacy.load(model, disable=[])
                return
            except Exception as e:
                last_err = e
                continue
        # if none loaded
        raise RuntimeError(f"No spaCy model found among {self.prefer_spacy_model_order}. Install one (e.g. python -m spacy download en_core_web_trf). Last error: {last_err}")

    def normalize_unicode(self, text: str) -> str:
        text = unicodedata.normalize("NFC", text)
        text = self.re_nonprint.sub(" ", text)
        return text

    def remove_addresses_heuristic(self, text: str) -> str:
        addr_pattern = re.compile(r'\b\d{1,5}\s+[A-Za-z0-9\.\-]+\s+(?:Street|St|Avenue|Ave|Road|Rd|Lane|Ln|Boulevard|Blvd|Drive|Dr|Court|Ct|Way|Place|Pl)\b.*?(?:,?\s*[A-Za-z\s]+,?\s*[A-Z]{2}\s*\d{5})?', re.IGNORECASE)
        text = addr_pattern.sub(self._mask_tokens['address'], text)
        text = self.re_zip.sub(self._mask_tokens['address'], text)
        return text

    def apply_extra_removals(self, text: str, extra_patterns: Optional[List[Pattern]] = None) -> str:
        patterns = list(self.default_extra_removals)
        if extra_patterns:
            patterns.extend(extra_patterns)
        for p in patterns:
            text = p.sub("", text)
        return text

    def preprocess_text(self, text: Optional[str],
                        lower: bool = True,
                        mask_emails: bool = True,
                        mask_phones: bool = True,
                        mask_urls: bool = True,
                        mask_ip: bool = True,
                        mask_credit: bool = True,
                        mask_ssn: bool = True,
                        remove_dates: bool = True,
                        remove_times: bool = True,
                        remove_punctuation: bool = False,
                        remove_non_alpha_tokens: bool = False,
                        keep_case_ids: bool = True,
                        mask_case_ids: bool = False,
                        remove_addresses: bool = True,
                        use_spacy_ner: bool = False,
                        lemmatize: bool = False,
                        remove_stopwords: bool = False,
                        custom_stopwords: Optional[set] = None,
                        keep_numbers: bool = True,
                        extra_removals: Optional[List[Pattern]] = None
                        ) -> str:
        """
        Process a single text string.
        - keep_numbers: if True, numeric tokens are preserved. If False, numeric-only tokens may be removed.
        - lemmatize/remove_stopwords require spaCy.
        - extra_removals: list of compiled regex patterns to remove from text (for screenshot notes etc).
        """

        if text is None:
            return text
        if not isinstance(text, str):
            text = str(text)

        # normalize and strip extra screenshot/UI artifacts early
        text = self.normalize_unicode(text)
        text = self.apply_extra_removals(text, extra_removals)

        # preserve case ids by replacing with placeholders
        case_id_tokens = {}
        if keep_case_ids:
            def _case_repl(m):
                tok = f"__CASEID_{len(case_id_tokens)}__"
                case_id_tokens[tok] = m.group(0)
                return tok
            text = self.re_case_id.sub(_case_repl, text)
        elif mask_case_ids:
            text = self.re_case_id.sub(self._mask_tokens['case_id'], text)

        # mask basic patterns first (so NER/lemmatization won't split them)
        if mask_urls:
            text = self.re_url2.sub(self._mask_tokens['url'], text)
        if mask_emails:
            text = self.re_email.sub(self._mask_tokens['email'], text)
        if mask_phones:
            text = self.re_phone.sub(self._mask_tokens['phone'], text)
        if mask_ip:
            text = self.re_ip.sub(self._mask_tokens['ip'], text)
        if mask_credit:
            text = self.re_credit.sub(self._mask_tokens['credit'], text)
        if mask_ssn:
            text = self.re_ssn.sub(self._mask_tokens['ssn'], text)
        if remove_dates:
            text = self.re_datetime.sub(self._mask_tokens['date'], text)
        if remove_times:
            text = self.re_time.sub(self._mask_tokens['time'], text)
        if remove_addresses:
            text = self.remove_addresses_heuristic(text)

        # If spaCy based ops requested, prepare doc
        doc = None
        use_spacy = (use_spacy_ner or lemmatize or remove_stopwords)
        if use_spacy:
            try:
                self._ensure_spacy()
                doc = self.nlp(text)
            except RuntimeError:
                # spaCy missing; continue without spacy operations
                doc = None
                use_spacy = False

        # If using spaCy, do token-level ops: NER masking, lemmatization, stopword removal
        if use_spacy and doc is not None:
            out_tokens = []
            custom_stopwords = custom_stopwords or set()
            for token in doc:
                t = token.text
                # preserve our placeholders / case-id placeholders as-is
                if t in self._mask_tokens.values() or t.startswith("__CASEID_"):
                    out_tokens.append(t)
                    continue

                # NER masking if needed (work on token.ent_type_)
                if use_spacy_ner and token.ent_type_ in ("PERSON", "ORG", "GPE", "LOC", "FAC"):
                    ent_label = token.ent_type_
                    if ent_label == "PERSON":
                        out_tokens.append(self._mask_tokens['person'])
                    elif ent_label == "ORG":
                        out_tokens.append(self._mask_tokens['org'])
                    else:
                        out_tokens.append(self._mask_tokens['location'])
                    continue

                # Stopword removal
                if remove_stopwords:
                    # use token.is_stop or custom stopwords (check lemma + lower)
                    lemma_low = token.lemma_.lower() if token.lemma_ else token.text.lower()
                    if token.is_stop or lemma_low in custom_stopwords or token.text.lower() in custom_stopwords:
                        continue

                # Lemmatize if requested
                if lemmatize:
                    tok_text = token.lemma_
                else:
                    tok_text = token.text

                # Keep numbers if configured
                if not keep_numbers:
                    if tok_text.isdigit():
                        continue

                # Optionally drop non-alpha tokens
                if remove_non_alpha_tokens:
                    alpha_chars = sum(1 for ch in tok_text if ch.isalpha())
                    if len(tok_text) > 0 and (alpha_chars / max(1, len(tok_text))) < 0.5:
                        # keep placeholders or case ids
                        if tok_text in self._mask_tokens.values() or tok_text.startswith("__CASEID_"):
                            out_tokens.append(tok_text)
                        else:
                            continue

                out_tokens.append(tok_text)
            text = " ".join(out_tokens)
        else:
            # Non-spacy fallback: basic punctuation removal or simple token filter
            if remove_punctuation:
                text = self.re_punct.sub(" ", text)
            if remove_non_alpha_tokens:
                toks = text.split()
                filtered = []
                for t in toks:
                    alpha_chars = sum(1 for ch in t if ch.isalpha())
                    if len(t) == 0:
                        continue
                    if alpha_chars / max(1, len(t)) < 0.5:
                        if t in case_id_tokens or t in self._mask_tokens.values():
                            filtered.append(t)
                        else:
                            continue
                    else:
                        filtered.append(t)
                text = " ".join(filtered)

        # restore preserved case ids
        if case_id_tokens:
            for placeholder, orig in case_id_tokens.items():
                text = text.replace(placeholder, orig)

        # final cleanup of extra whitespace
        text = re.sub(r'\s{2,}', ' ', text).strip()
        if lower:
            text = text.lower()
        return text

    def preprocess_dataframe(self, df: pd.DataFrame,
                             columns: List[str],
                             inplace: bool = False,
                             output_csv: Optional[str] = None,
                             csv_index: bool = False,
                             **kwargs) -> Tuple[pd.DataFrame, Dict[str, List[Tuple[Any, str, str]]]]:
        """
        Apply preprocess_text to all specified columns.
        - output_csv: if provided, will save the processed DataFrame to this CSV path.
        - csv_index: whether to write the DataFrame index into CSV.
        - kwargs forwarded to preprocess_text
        Returns (processed_df, replacements_log)
        """
        if not inplace:
            df_proc = df.copy()
        else:
            df_proc = df

        replacements_log: Dict[str, List[Tuple[Any, str, str]]] = {col: [] for col in columns}

        for col in columns:
            if col not in df_proc.columns:
                raise KeyError(f"Column '{col}' not in DataFrame")
            for idx, orig in df_proc[col].items():
                try:
                    processed = self.preprocess_text(orig, **kwargs)
                except Exception as e:
                    processed = orig
                df_proc.at[idx, col] = processed
                replacements_log[col].append((idx, orig, processed))

        if output_csv:
            df_proc.to_csv(output_csv, index=csv_index, encoding='utf-8')

        return df_proc, replacements_log


# -----------------------
# Example usage (demo)
# -----------------------
if __name__ == "__main__":
    demo = pd.DataFrame({
        "id": [1, 2, 3],
        "text": [
            "Problem Description is clean. Serial: FRA21-0101-0400-05T2. Contact: Michael.Brawner@microsoft.com +1 (703) 234-2526. 41840 Growth Mindset Lane, Aldie, VA 20105. Requested Delivery Date: August 18. Ln 9, Col 54",
            "Overlapping data, dashes --- timestamps 2023-07-15 10:34:22 and url http://example.com. Case# CASE-2024-ABC-002. this is Date should be removed look at some solution",
            "Address: 1234 Elm St, Smalltown, CA 90210. Email: someone@company.org. IP: 192.168.0.1. CC: 4111 1111 1111 1111. 1:26 PM 9/19/2025"
        ]
    })

    tp = TextPreprocessorV2()
    processed_df, log = tp.preprocess_dataframe(
        demo,
        columns=["text"],
        inplace=False,
        output_csv="processed_demo.csv",  # CSV saved
        csv_index=False,
        lower=False,                # keep casing for case ids or human readability
        mask_emails=True,
        mask_phones=True,
        mask_urls=True,
        mask_ip=True,
        mask_credit=True,
        mask_ssn=True,
        remove_dates=True,
        remove_times=True,
        remove_punctuation=False,
        remove_non_alpha_tokens=False,
        keep_case_ids=True,
        mask_case_ids=False,
        remove_addresses=True,
        use_spacy_ner=True,        # enable spaCy-based NER masking
        lemmatize=True,            # enable lemmatization
        remove_stopwords=True,     # remove stopwords
        custom_stopwords={"requested", "delivery"},
        keep_numbers=True,         # IMPORTANT: keep numeric tokens by default
        extra_removals=[
            re.compile(r'\bLn\s*\d+,\s*Col\s*\d+', re.IGNORECASE),
            re.compile(r'this is Date should be removed[^\n]*', re.IGNORECASE),
        ]
    )

    print(processed_df.to_dict(orient='records'))
    # processed_demo.csv is written in current folder


---

Quick notes & guidance

1. Installing spaCy + advanced model

Advanced transformer model (en_core_web_trf) is recommended for best NER/lemmatization quality but is large and requires PyTorch/Transformers backend. If you want a lighter install, use en_core_web_lg or en_core_web_sm.

Example install commands:

pip install spacy

python -m spacy download en_core_web_trf  (or en_core_web_lg / en_core_web_sm)


If you don't have the transformer model installed, the code will try fallbacks (lg → md → sm) or raise an instructive error.



2. Keeping numbers

The new option keep_numbers=True prevents accidental removal of numeric tokens (serials, dates, metrics). Set to False to drop numeric-only tokens.



3. Custom stopwords

Pass a Python set of custom stopwords via custom_stopwords={"word1","word2"} — they'll be matched against lemma and lowercase forms.



4. Extra/screenshot-specific removals

I added default_extra_removals to remove obvious UI artifacts from your screenshot (e.g. Ln 9, Col 54, Windows (CRLF), short dates/times and the explicit phrase this is Date should be removed...).

You can provide extra_removals (list of compiled re patterns) into preprocess_dataframe for screenshot-specific lines.



5. CSV output

Provide output_csv="path/processed.csv" to preprocess_dataframe to write the resulting DataFrame.



6. Audit log

Function returns replacements_log mapping each column to a list of (index, original, processed) tuples so you can inspect what changed and why.





---

If you want, I can:

Trim the default extra_removals to more exact patterns from the screenshot text if you want me to extract a precise list (I already included a few obvious ones).

Produce a smaller lighter script for environments where spaCy transformer model is not feasible.

Run a quick test on your actual CSV/DF if you paste a small sample (I’ll show before/after and the CSV contents).


