{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dfbcdd0-e9e2-488d-936a-863e1a0f5ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and logging setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import Dict, Any, TypedDict, List\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"Missing GROQ_API_KEY in environment variables. Please set it in .env file.\")\n",
    "\n",
    "print(\"Imports and logging setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eed5c5e-69b4-4e95-a43e-7337698fc8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM and Embeddings initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM and Embedding Model\n",
    "llm = ChatGroq(\n",
    "    temperature=0.8,\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    groq_api_key=GROQ_API_KEY,\n",
    "    max_tokens=2000\n",
    ")\n",
    "# embedder = HuggingFaceEmbeddings(\n",
    "#     model_name=\"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "#     model_kwargs={\"device\": \"cpu\"}\n",
    "# )\n",
    "\n",
    "print(\"LLM and Embeddings initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea2fc37-1217-40be-8bf1-7979c0241455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State and Prompt Library defined.\n"
     ]
    }
   ],
   "source": [
    "# State Definition\n",
    "class ChatbotState(TypedDict):\n",
    "    query: str\n",
    "    retrieved_docs: List[str]\n",
    "    response: str\n",
    "    metadata: Dict[str, Any]\n",
    "    prompt_type: str\n",
    "    conversation_history: List[Dict[str, str]]\n",
    "    requires_human_review: bool\n",
    "\n",
    "# Prompt Library with Advanced Techniques\n",
    "class PromptLibrary:\n",
    "    def __init__(self):\n",
    "        self.prompts = {\n",
    "            \"qa\": {\n",
    "                \"template\": \"\"\"Answer '{question}' using this context: {context}\n",
    "- If the context fully answers the question, provide a concise, accurate response with key details highlighted.\n",
    "- If the context is partial, give a brief answer based on available info and note what’s missing.\n",
    "- If no relevant info is found, say: \"The document doesn’t provide enough information to answer this.\"\n",
    "Focus on clarity, relevance, and actionable insights.\"\"\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"tags\": [\"qa\", \"least-to-most\"],\n",
    "                \"description\": \"Breaks down questions systematically.\"\n",
    "            },\n",
    "            \"summarization\": {\n",
    "                \"template\": \"\"\"Summarize this document content: {context}\n",
    "- Identify and prioritize key sections (e.g., personal info, skills, experience, projects).\n",
    "- Provide a concise, structured summary (e.g., bullet points or short paragraphs) that captures the essence.\n",
    "- Exclude irrelevant details and focus on what makes the content unique or valuable.\n",
    "Aim for clarity, brevity, and a professional tone.\"\"\",\n",
    "                \"version\": \"1.1\",\n",
    "                \"tags\": [\"summarization\", \"self-refinement\"],\n",
    "                \"description\": \"Self-refines for brevity.\"\n",
    "            },\n",
    "            \"creative\": {\n",
    "                \"template\": \"\"\"Generate a creative response to '{question}' using context: {context}\n",
    "Focus on inspiration and positivity.\"\"\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"tags\": [\"creative\", \"directional-stimulus\"],\n",
    "                \"description\": \"Produces uplifting responses.\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def get_prompt(self, prompt_type: str, query: str, context: str) -> PromptTemplate:\n",
    "        if prompt_type not in self.prompts:\n",
    "            logger.warning(f\"Prompt type '{prompt_type}' not found, defaulting to 'qa'\")\n",
    "            prompt_type = \"qa\"\n",
    "        data = self.prompts[prompt_type]\n",
    "        return PromptTemplate(\n",
    "            template=data[\"template\"],\n",
    "            input_variables=[\"context\", \"question\"] if \"question\" in data[\"template\"] else [\"context\"],\n",
    "            metadata={\"version\": data[\"version\"], \"tags\": data[\"tags\"]}\n",
    "        )\n",
    "\n",
    "        \n",
    "    def route_prompt(self, query: str) -> str:\n",
    "        # Convert query to lowercase for case-insensitive matching\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Keyword-based intent detection\n",
    "            # Fallback to LLM-based classification if keywords don't match\n",
    "        messages = [\n",
    "            SystemMessage(content=\"If user is asking question about documnet then classisfy intent as 'qa' . If user is asking about to summarize then do 'summarization' .Classify the intent as 'qa', 'summarization' only. Provide only the intent name.\"),\n",
    "            HumanMessage(content=query)\n",
    "        ]\n",
    "        intent = llm.invoke(messages).content.strip().lower()\n",
    "        return intent if intent in self.prompts else \"qa\"\n",
    "\n",
    "\n",
    "prompt_lib = PromptLibrary()\n",
    "print(\"State and Prompt Library defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b070f-9b4e-4713-9ba9-16e1913eca74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0737e1-90eb-4408-bf76-019ed0893df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04f5a1a-ce93-4ad3-8169-af447451c724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 17:40:25,515 - INFO - Use pytorch device_name: cpu\n",
      "2025-03-25 17:40:25,516 - INFO - Load pretrained SentenceTransformer: sentence-transformers/stsb-xlm-r-multilingual\n",
      "2025-03-25 17:40:30,914 - INFO - Initialized advanced embedder: stsb-xlm-r-multilingual (768 dimensions)\n",
      "2025-03-25 17:40:31,885 - INFO - Use pytorch device: cpu\n",
      "2025-03-25 17:40:31,885 - INFO - Initialized CrossEncoder: ms-marco-MiniLM-L-6-v2 for reranking\n",
      "2025-03-25 17:40:31,885 - INFO - Discovering subpackages in _NamespacePath(['C:\\\\Users\\\\Yash\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\env_ai\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "2025-03-25 17:40:31,885 - INFO - Looking for plugins in pinecone_plugins.inference\n",
      "2025-03-25 17:40:31,931 - INFO - Installing plugin inference into Pinecone\n",
      "2025-03-25 17:40:33,477 - INFO - Index rag-index already exists.\n",
      "2025-03-25 17:40:34,209 - INFO - Discovering subpackages in _NamespacePath(['C:\\\\Users\\\\Yash\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\env_ai\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "2025-03-25 17:40:34,209 - INFO - Looking for plugins in pinecone_plugins.inference\n",
      "2025-03-25 17:40:34,209 - INFO - Connected to Pinecone index: rag-index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG System with Pinecone initialized (768 dimensions).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, UnstructuredXMLLoader\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"Missing PINECONE_API_KEY in environment variables. Please set it in .env file.\")\n",
    "\n",
    "# Configure logging with detailed format and file output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"rag_system.log\", mode='a'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AdvancedEmbedder:\n",
    "    def __init__(self):\n",
    "        # Using a multilingual model with 768 dimensions\n",
    "        self.model = SentenceTransformer(\"sentence-transformers/stsb-xlm-r-multilingual\")\n",
    "        logger.info(\"Initialized advanced embedder: stsb-xlm-r-multilingual (768 dimensions)\")\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        start_time = time.time()\n",
    "        embeddings = self.model.encode(texts, batch_size=32, show_progress_bar=False)\n",
    "        logger.info(f\"Embedded {len(texts)} documents in {time.time() - start_time:.2f} seconds\")\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        start_time = time.time()\n",
    "        embedding = self.model.encode([query])[0]\n",
    "        logger.info(f\"Embedded query in {time.time() - start_time:.2f} seconds\")\n",
    "        return embedding.tolist()\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, index_name=\"rag-index\", dimension=768, cloud=\"aws\", region=\"us-east-1\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with Pinecone and advanced components.\n",
    "        \n",
    "        Args:\n",
    "            index_name (str): Name of the Pinecone index.\n",
    "            dimension (int): Dimension of the embedding vectors (768 for stsb-xlm-r-multilingual).\n",
    "            cloud (str): Cloud provider (e.g., \"aws\", \"gcp\", \"azure\").\n",
    "            region (str): Region for the cloud provider (e.g., \"us-east-1\" for AWS).\n",
    "        \"\"\"\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=700,\n",
    "            chunk_overlap=150,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        self.embeddings = AdvancedEmbedder()\n",
    "        self.reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")  # Initialized once\n",
    "        logger.info(\"Initialized CrossEncoder: ms-marco-MiniLM-L-6-v2 for reranking\")\n",
    "        self.index_name = index_name\n",
    "        self.dimension = dimension\n",
    "        self.cloud = cloud\n",
    "        self.region = region\n",
    "        self.pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        self.index = None\n",
    "        self._initialize_pinecone()\n",
    "\n",
    "    def _initialize_pinecone(self) -> None:\n",
    "        \"\"\"Initialize or connect to a Pinecone index with region validation.\"\"\"\n",
    "        supported_regions = {\n",
    "            \"aws\": [\"us-east-1\", \"us-west-2\", \"eu-west-1\"],\n",
    "            \"gcp\": [\"us-central1\", \"europe-west1\"],\n",
    "            \"azure\": [\"eastus\"]\n",
    "        }\n",
    "        if self.cloud not in supported_regions:\n",
    "            raise ValueError(f\"Unsupported cloud provider: {self.cloud}. Supported: {list(supported_regions.keys())}\")\n",
    "        if self.region not in supported_regions[self.cloud]:\n",
    "            raise ValueError(f\"Region {self.region} not supported for {self.cloud}. Supported: {supported_regions[self.cloud]}\")\n",
    "\n",
    "        if self.index_name not in self.pc.list_indexes().names():\n",
    "            try:\n",
    "                self.pc.create_index(\n",
    "                    name=self.index_name,\n",
    "                    dimension=self.dimension,\n",
    "                    metric=\"cosine\",\n",
    "                    spec=ServerlessSpec(cloud=self.cloud, region=self.region)\n",
    "                )\n",
    "                logger.info(f\"Created new Pinecone index: {self.index_name} with dimension {self.dimension}\")\n",
    "                time.sleep(5)  # Wait for index creation to propagate\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to create index {self.index_name}: {str(e)}\")\n",
    "                raise\n",
    "        else:\n",
    "            logger.info(f\"Index {self.index_name} already exists.\")\n",
    "\n",
    "        self.index = self.pc.Index(self.index_name)\n",
    "        logger.info(f\"Connected to Pinecone index: {self.index_name}\")\n",
    "\n",
    "    def _parallel_upsert(self, vectors: List[dict], namespace: str, batch_size=100) -> None:\n",
    "        \"\"\"Perform parallel upsert for faster indexing with detailed logging.\"\"\"\n",
    "        def upsert_batch(batch):\n",
    "            start_time = time.time()\n",
    "            self.index.upsert(vectors=batch, namespace=namespace)\n",
    "            logger.info(\n",
    "                f\"Upserted batch of {len(batch)} vectors in {time.time() - start_time:.2f} seconds \"\n",
    "                f\"(namespace: {namespace}, IDs: {[v['id'] for v in batch]})\"\n",
    "            )\n",
    "\n",
    "        total_chunks = len(vectors)\n",
    "        logger.info(f\"Starting parallel upsert of {total_chunks} vectors into namespace: {namespace}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            for i in range(0, len(vectors), batch_size):\n",
    "                batch = vectors[i:i + batch_size]\n",
    "                executor.submit(upsert_batch, batch)\n",
    "\n",
    "        logger.info(f\"Completed upsert of {total_chunks} vectors in {time.time() - start_time:.2f} seconds\")\n",
    "        time.sleep(2)  # Wait for upsert to propagate\n",
    "\n",
    "    def load_document(self, file_path: str, namespace: str = \"default\") -> None:\n",
    "        \"\"\"Load and process a document with advanced strategies and detailed logging.\"\"\"\n",
    "        file_path = os.path.normpath(file_path.strip())\n",
    "        if not os.path.isfile(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        loaders = {\n",
    "            \".pdf\": PyPDFLoader,\n",
    "            \".docx\": Docx2txtLoader,\n",
    "            \".txt\": TextLoader,\n",
    "            \".xml\": UnstructuredXMLLoader\n",
    "        }\n",
    "        if ext not in loaders:\n",
    "            raise ValueError(f\"Unsupported file type: {ext}. Supported: {list(loaders.keys())}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        loader = loaders[ext](file_path)\n",
    "        docs = loader.load()\n",
    "        logger.info(f\"Loaded document: {file_path} in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        chunks = self.text_splitter.split_documents(docs)\n",
    "        chunk_texts = [doc.page_content for doc in chunks]\n",
    "        logger.info(f\"Split document into {len(chunks)} chunks in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        embeddings = self.embeddings.embed_documents(chunk_texts)\n",
    "        vectors = []\n",
    "        start_time = time.time()\n",
    "        for i, (text, embedding) in enumerate(zip(chunk_texts, embeddings)):\n",
    "            vector_id = f\"{file_path}_{i}\"\n",
    "            vectors.append({\"id\": vector_id, \"values\": embedding, \"metadata\": {\"text\": text}})\n",
    "            if i < 3:\n",
    "                logger.info(\n",
    "                    f\"Created vector {i+1}/{len(chunk_texts)}: ID={vector_id}, \"\n",
    "                    f\"Embedding length={len(embedding)}, Sample text={text[:50]}...\"\n",
    "                )\n",
    "        logger.info(f\"Generated {len(vectors)} vectors in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        self._parallel_upsert(vectors, namespace)\n",
    "        logger.info(f\"Completed loading and upserting {file_path} into namespace: {namespace}\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 6, namespace: str = \"default\", initial_k: int = 20) -> List[str]:\n",
    "        \"\"\"Retrieve relevant document chunks using advanced techniques like query expansion and reranking.\"\"\"\n",
    "        if not self.index:\n",
    "            logger.warning(\"No Pinecone index initialized.\")\n",
    "            return []\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Step 1: Query Expansion - Generate a richer query\n",
    "        expanded_query = self._expand_query(query)\n",
    "        logger.info(f\"Expanded query: {expanded_query}\")\n",
    "\n",
    "        # Step 2: Initial Retrieval - Fetch more candidates than needed for reranking\n",
    "        query_embedding = self.embeddings.embed_query(expanded_query)\n",
    "        initial_result = self.index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=initial_k,\n",
    "            include_metadata=True,\n",
    "            namespace=namespace\n",
    "        )\n",
    "        initial_docs = [\n",
    "            {\"text\": match[\"metadata\"][\"text\"], \"score\": match[\"score\"]}\n",
    "            for match in initial_result[\"matches\"]\n",
    "        ]\n",
    "        logger.info(f\"Initially retrieved {len(initial_docs)} chunks in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        if not initial_docs:\n",
    "            logger.info(\"No documents retrieved from initial query.\")\n",
    "            return []\n",
    "\n",
    "        # Step 3: Reranking - Use pre-initialized CrossEncoder\n",
    "        reranked_docs = self._rerank_docs(query, initial_docs)\n",
    "        \n",
    "        # Step 4: Select top-k reranked documents\n",
    "        final_docs = [doc[\"text\"] for doc in reranked_docs[:k]]\n",
    "        logger.info(f\"Reranked and selected top {len(final_docs)} chunks in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        return final_docs\n",
    "\n",
    "    def _expand_query(self, query: str) -> str:\n",
    "        \"\"\"Expand the query using synonyms or context to improve retrieval recall.\"\"\"\n",
    "        expansion_terms = {\n",
    "            \"project\": \"work experience task\",\n",
    "            \"skill\": \"ability expertise\",\n",
    "            \"summary\": \"overview brief\",\n",
    "            \"what\": \"details information\"\n",
    "        }\n",
    "        query_lower = query.lower()\n",
    "        expanded = query\n",
    "        for term, synonyms in expansion_terms.items():\n",
    "            if term in query_lower:\n",
    "                expanded += f\" {synonyms}\"\n",
    "                break  # Simple: expand only once for the first match\n",
    "        \n",
    "        return expanded.strip()\n",
    "\n",
    "    def _rerank_docs(self, query: str, docs: List[Dict[str, float]]) -> List[Dict[str, float]]:\n",
    "        \"\"\"Rerank retrieved documents using the pre-initialized CrossEncoder.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Reranking with pre-initialized CrossEncoder\")\n",
    "            pairs = [[query, doc[\"text\"]] for doc in docs]\n",
    "            scores = self.reranker.predict(pairs)\n",
    "            for i, doc in enumerate(docs):\n",
    "                doc[\"rerank_score\"] = float(scores[i])\n",
    "            reranked_docs = sorted(docs, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "            logger.info(f\"Reranked {len(reranked_docs)} documents\")\n",
    "            return reranked_docs\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Reranking failed: {str(e)}. Falling back to initial ranking.\")\n",
    "            return sorted(docs, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    def clear(self, namespace: str = \"default\") -> None:\n",
    "        \"\"\"Clear a namespace in the Pinecone index.\"\"\"\n",
    "        if self.index:\n",
    "            start_time = time.time()\n",
    "            self.index.delete(delete_all=True, namespace=namespace)\n",
    "            logger.info(f\"Cleared Pinecone namespace: {namespace} in {time.time() - start_time:.2f} seconds\")\n",
    "        else:\n",
    "            logger.warning(\"No Pinecone index to clear.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the RAG system\n",
    "    rag = RAGSystem(index_name=\"rag-index\", dimension=768, cloud=\"aws\", region=\"us-east-1\")\n",
    "    print(\"RAG System with Pinecone initialized (768 dimensions).\")\n",
    "\n",
    "    # Example usage (uncomment to test)\n",
    "    # rag.load_document(\"sample.pdf\", namespace=\"docs\")\n",
    "    # results = rag.retrieve(\"What are the project details?\", k=3, namespace=\"docs\")\n",
    "    # for i, result in enumerate(results):\n",
    "    #     print(f\"Result {i+1}: {result[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc23397-41a5-4601-b642-c403ef331a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d7a27-e541-46c1-b720-ff087e6909d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17fd3b0-8fee-48bf-b8d2-37e8dbe6b8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8892936-5fbe-4d61-a213-9f86b680327f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af1b7a-c18b-42dd-95aa-aadc9fbf84e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35c597-e9ae-471d-97ef-83244295be93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8e2724-8a5a-48c9-bf57-58dd2a2bcc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import logging\n",
    "# from typing import List, Dict\n",
    "# from dotenv import load_dotenv\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, UnstructuredXMLLoader\n",
    "# from sentence_transformers import SentenceTransformer, CrossEncoder  # Added CrossEncoder import\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# import time\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "# PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "# if not PINECONE_API_KEY:\n",
    "#     raise ValueError(\"Missing PINECONE_API_KEY in environment variables. Please set it in .env file.\")\n",
    "\n",
    "# # Configure logging with detailed format and file output\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',\n",
    "#     handlers=[\n",
    "#         logging.FileHandler(\"rag_system.log\", mode='a'),\n",
    "#         logging.StreamHandler()\n",
    "#     ]\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# class AdvancedEmbedder:\n",
    "#     def __init__(self):\n",
    "#         # Using a multilingual model with 768 dimensions\n",
    "#         self.model = SentenceTransformer(\"sentence-transformers/stsb-xlm-r-multilingual\")\n",
    "#         logger.info(\"Initialized advanced embedder: stsb-xlm-r-multilingual (768 dimensions)\")\n",
    "\n",
    "#     def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "#         start_time = time.time()\n",
    "#         embeddings = self.model.encode(texts, batch_size=32, show_progress_bar=False)\n",
    "#         logger.info(f\"Embedded {len(texts)} documents in {time.time() - start_time:.2f} seconds\")\n",
    "#         return embeddings.tolist()\n",
    "\n",
    "#     def embed_query(self, query: str) -> List[float]:\n",
    "#         start_time = time.time()\n",
    "#         embedding = self.model.encode([query])[0]\n",
    "#         logger.info(f\"Embedded query in {time.time() - start_time:.2f} seconds\")\n",
    "#         return embedding.tolist()\n",
    "\n",
    "# class RAGSystem:\n",
    "#     def __init__(self, index_name=\"rag-index\", dimension=768, cloud=\"aws\", region=\"us-east-1\"):\n",
    "#         \"\"\"\n",
    "#         Initialize the RAG system with Pinecone and advanced components.\n",
    "        \n",
    "#         Args:\n",
    "#             index_name (str): Name of the Pinecone index.\n",
    "#             dimension (int): Dimension of the embedding vectors (768 for stsb-xlm-r-multilingual).\n",
    "#             cloud (str): Cloud provider (e.g., \"aws\", \"gcp\", \"azure\").\n",
    "#             region (str): Region for the cloud provider (e.g., \"us-east-1\" for AWS).\n",
    "#         \"\"\"\n",
    "#         self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "#             chunk_size=700,\n",
    "#             chunk_overlap=150,\n",
    "#             length_function=len,\n",
    "#             separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "#         )\n",
    "#         self.embeddings = AdvancedEmbedder()\n",
    "#         self.index_name = index_name\n",
    "#         self.dimension = dimension\n",
    "#         self.cloud = cloud\n",
    "#         self.region = region\n",
    "#         self.pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "#         self.index = None\n",
    "#         self._initialize_pinecone()\n",
    "\n",
    "#     def _initialize_pinecone(self) -> None:\n",
    "#         \"\"\"Initialize or connect to a Pinecone index with region validation.\"\"\"\n",
    "#         supported_regions = {\n",
    "#             \"aws\": [\"us-east-1\", \"us-west-2\", \"eu-west-1\"],\n",
    "#             \"gcp\": [\"us-central1\", \"europe-west1\"],\n",
    "#             \"azure\": [\"eastus\"]\n",
    "#         }\n",
    "#         if self.cloud not in supported_regions:\n",
    "#             raise ValueError(f\"Unsupported cloud provider: {self.cloud}. Supported: {list(supported_regions.keys())}\")\n",
    "#         if self.region not in supported_regions[self.cloud]:\n",
    "#             raise ValueError(f\"Region {self.region} not supported for {self.cloud}. Supported: {supported_regions[self.cloud]}\")\n",
    "\n",
    "#         if self.index_name not in self.pc.list_indexes().names():\n",
    "#             try:\n",
    "#                 self.pc.create_index(\n",
    "#                     name=self.index_name,\n",
    "#                     dimension=self.dimension,\n",
    "#                     metric=\"cosine\",\n",
    "#                     spec=ServerlessSpec(cloud=self.cloud, region=self.region)\n",
    "#                 )\n",
    "#                 logger.info(f\"Created new Pinecone index: {self.index_name} with dimension {self.dimension}\")\n",
    "#                 time.sleep(5)  # Wait for index creation to propagate\n",
    "#             except Exception as e:\n",
    "#                 logger.error(f\"Failed to create index {self.index_name}: {str(e)}\")\n",
    "#                 raise\n",
    "#         else:\n",
    "#             logger.info(f\"Index {self.index_name} already exists.\")\n",
    "\n",
    "#         self.index = self.pc.Index(self.index_name)\n",
    "#         logger.info(f\"Connected to Pinecone index: {self.index_name}\")\n",
    "\n",
    "#     def _parallel_upsert(self, vectors: List[dict], namespace: str, batch_size=100) -> None:\n",
    "#         \"\"\"Perform parallel upsert for faster indexing with detailed logging.\"\"\"\n",
    "#         def upsert_batch(batch):\n",
    "#             start_time = time.time()\n",
    "#             self.index.upsert(vectors=batch, namespace=namespace)\n",
    "#             logger.info(\n",
    "#                 f\"Upserted batch of {len(batch)} vectors in {time.time() - start_time:.2f} seconds \"\n",
    "#                 f\"(namespace: {namespace}, IDs: {[v['id'] for v in batch]})\"\n",
    "#             )\n",
    "\n",
    "#         total_chunks = len(vectors)\n",
    "#         logger.info(f\"Starting parallel upsert of {total_chunks} vectors into namespace: {namespace}\")\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#             for i in range(0, len(vectors), batch_size):\n",
    "#                 batch = vectors[i:i + batch_size]\n",
    "#                 executor.submit(upsert_batch, batch)\n",
    "\n",
    "#         logger.info(f\"Completed upsert of {total_chunks} vectors in {time.time() - start_time:.2f} seconds\")\n",
    "#         time.sleep(2)  # Wait for upsert to propagate\n",
    "\n",
    "#     def load_document(self, file_path: str, namespace: str = \"default\") -> None:\n",
    "#         \"\"\"Load and process a document with advanced strategies and detailed logging.\"\"\"\n",
    "#         file_path = os.path.normpath(file_path.strip())\n",
    "#         if not os.path.isfile(file_path):\n",
    "#             raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "#         ext = os.path.splitext(file_path)[1].lower()\n",
    "#         loaders = {\n",
    "#             \".pdf\": PyPDFLoader,\n",
    "#             \".docx\": Docx2txtLoader,\n",
    "#             \".txt\": TextLoader,\n",
    "#             \".xml\": UnstructuredXMLLoader\n",
    "#         }\n",
    "#         if ext not in loaders:\n",
    "#             raise ValueError(f\"Unsupported file type: {ext}. Supported: {list(loaders.keys())}\")\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         loader = loaders[ext](file_path)\n",
    "#         docs = loader.load()\n",
    "#         logger.info(f\"Loaded document: {file_path} in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         chunks = self.text_splitter.split_documents(docs)\n",
    "#         chunk_texts = [doc.page_content for doc in chunks]\n",
    "#         logger.info(f\"Split document into {len(chunks)} chunks in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "#         embeddings = self.embeddings.embed_documents(chunk_texts)\n",
    "#         vectors = []\n",
    "#         start_time = time.time()\n",
    "#         for i, (text, embedding) in enumerate(zip(chunk_texts, embeddings)):\n",
    "#             vector_id = f\"{file_path}_{i}\"\n",
    "#             vectors.append({\"id\": vector_id, \"values\": embedding, \"metadata\": {\"text\": text}})\n",
    "#             if i < 3:\n",
    "#                 logger.info(\n",
    "#                     f\"Created vector {i+1}/{len(chunk_texts)}: ID={vector_id}, \"\n",
    "#                     f\"Embedding length={len(embedding)}, Sample text={text[:50]}...\"\n",
    "#                 )\n",
    "#         logger.info(f\"Generated {len(vectors)} vectors in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "#         self._parallel_upsert(vectors, namespace)\n",
    "#         logger.info(f\"Completed loading and upserting {file_path} into namespace: {namespace}\")\n",
    "\n",
    "#     def retrieve(self, query: str, k: int = 5, namespace: str = \"default\", initial_k: int = 20) -> List[str]:\n",
    "#         \"\"\"Retrieve relevant document chunks using advanced techniques like query expansion and reranking.\"\"\"\n",
    "#         if not self.index:\n",
    "#             logger.warning(\"No Pinecone index initialized.\")\n",
    "#             return []\n",
    "\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         # Step 1: Query Expansion - Generate a richer query using LLM (optional step)\n",
    "#         expanded_query = self._expand_query(query)\n",
    "#         logger.info(f\"Expanded query: {expanded_query}\")\n",
    "\n",
    "#         # Step 2: Initial Retrieval - Fetch more candidates than needed for reranking\n",
    "#         query_embedding = self.embeddings.embed_query(expanded_query)\n",
    "#         initial_result = self.index.query(\n",
    "#             vector=query_embedding,\n",
    "#             top_k=initial_k,  # Fetch more results initially (e.g., 20) for reranking\n",
    "#             include_metadata=True,\n",
    "#             namespace=namespace\n",
    "#         )\n",
    "#         initial_docs = [\n",
    "#             {\"text\": match[\"metadata\"][\"text\"], \"score\": match[\"score\"]}\n",
    "#             for match in initial_result[\"matches\"]\n",
    "#         ]\n",
    "#         logger.info(f\"Initially retrieved {len(initial_docs)} chunks in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "#         if not initial_docs:\n",
    "#             logger.info(\"No documents retrieved from initial query.\")\n",
    "#             return []\n",
    "\n",
    "#         # Step 3: Reranking - Use a CrossEncoder to score relevance more precisely\n",
    "#         reranked_docs = self._rerank_docs(query, initial_docs)\n",
    "        \n",
    "#         # Step 4: Select top-k reranked documents\n",
    "#         final_docs = [doc[\"text\"] for doc in reranked_docs[:k]]\n",
    "#         logger.info(f\"Reranked and selected top {len(final_docs)} chunks in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "#         return final_docs\n",
    "\n",
    "#     def _expand_query(self, query: str) -> str:\n",
    "#         \"\"\"Expand the query using synonyms or context to improve retrieval recall.\"\"\"\n",
    "#         expansion_terms = {\n",
    "#             \"project\": \"work experience task\",\n",
    "#             \"skill\": \"ability expertise\",\n",
    "#             \"summary\": \"overview brief\",\n",
    "#             \"what\": \"details information\"\n",
    "#         }\n",
    "#         query_lower = query.lower()\n",
    "#         expanded = query\n",
    "#         for term, synonyms in expansion_terms.items():\n",
    "#             if term in query_lower:\n",
    "#                 expanded += f\" {synonyms}\"\n",
    "#                 break  # Simple: expand only once for the first match\n",
    "        \n",
    "#         return expanded.strip()\n",
    "\n",
    "#     def _rerank_docs(self, query: str, docs: List[Dict[str, float]]) -> List[Dict[str, float]]:\n",
    "#         \"\"\"Rerank retrieved documents using a CrossEncoder for better relevance.\"\"\"\n",
    "#         try:\n",
    "#             # Initialize CrossEncoder for reranking (e.g., a lightweight model)\n",
    "#             reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\") #! initiaize this model at begening only dont initialize it at start\n",
    "#             logger.info(\"Initialized CrossEncoder for reranking\")\n",
    "\n",
    "#             # Prepare query-document pairs for scoring\n",
    "#             pairs = [[query, doc[\"text\"]] for doc in docs]\n",
    "#             scores = reranker.predict(pairs)\n",
    "\n",
    "#             # Combine original docs with reranking scores\n",
    "#             for i, doc in enumerate(docs):\n",
    "#                 doc[\"rerank_score\"] = float(scores[i])  # Convert to float for consistency\n",
    "\n",
    "#             # Sort by rerank_score in descending order (higher score = more relevant)\n",
    "#             reranked_docs = sorted(docs, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "#             logger.info(f\"Reranked {len(reranked_docs)} documents\")\n",
    "#             return reranked_docs\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Reranking failed: {str(e)}. Falling back to initial ranking.\")\n",
    "#             # Fallback to original cosine similarity scores if reranking fails\n",
    "#             return sorted(docs, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "#     def clear(self, namespace: str = \"default\") -> None:\n",
    "#         \"\"\"Clear a namespace in the Pinecone index.\"\"\"\n",
    "#         if self.index:\n",
    "#             start_time = time.time()\n",
    "#             self.index.delete(delete_all=True, namespace=namespace)\n",
    "#             logger.info(f\"Cleared Pinecone namespace: {namespace} in {time.time() - start_time:.2f} seconds\")\n",
    "#         else:\n",
    "#             logger.warning(\"No Pinecone index to clear.\")\n",
    "\n",
    "# # Optional: Initialize a main RAG system instance\n",
    "# rag = RAGSystem(index_name=\"rag-index\", dimension=768, cloud=\"aws\", region=\"us-east-1\")\n",
    "# print(\"RAG System with Pinecone initialized (768 dimensions).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50b130-3f00-4c89-a79a-6c85ffa9ae44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e663a-6e03-4a8e-b942-adba15933dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a0c984-4d1a-48c6-9024-91a3c4c99156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import logging\n",
    "# from typing import List\n",
    "# from dotenv import load_dotenv\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, UnstructuredXMLLoader\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# import time\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "# PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "# if not PINECONE_API_KEY:\n",
    "#     raise ValueError(\"Missing PINECONE_API_KEY in environment variables. Please set it in .env file.\")\n",
    "\n",
    "# # Configure logging with detailed format and file output\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',\n",
    "#     handlers=[\n",
    "#         logging.FileHandler(\"rag_system.log\", mode='a'),\n",
    "#         logging.StreamHandler()\n",
    "#     ]\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# class AdvancedEmbedder:\n",
    "#     def __init__(self):\n",
    "#         # Using a multilingual model with 768 dimensions\n",
    "#         self.model = SentenceTransformer(\"sentence-transformers/stsb-xlm-r-multilingual\")\n",
    "#         logger.info(\"Initialized advanced embedder: stsb-xlm-r-multilingual (768 dimensions)\")\n",
    "\n",
    "#     def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "#         start_time = time.time()\n",
    "#         embeddings = self.model.encode(texts, batch_size=32, show_progress_bar=False)\n",
    "#         logger.info(f\"Embedded {len(texts)} documents in {time.time() - start_time:.2f} seconds\")\n",
    "#         return embeddings.tolist()\n",
    "\n",
    "#     def embed_query(self, query: str) -> List[float]:\n",
    "#         start_time = time.time()\n",
    "#         embedding = self.model.encode([query])[0]\n",
    "#         logger.info(f\"Embedded query in {time.time() - start_time:.2f} seconds\")\n",
    "#         return embedding.tolist()\n",
    "\n",
    "# class RAGSystem:\n",
    "#     def __init__(self, index_name=\"rag-index\", dimension=768, cloud=\"aws\", region=\"us-east-1\"):\n",
    "#         \"\"\"\n",
    "#         Initialize the RAG system with Pinecone and advanced components.\n",
    "        \n",
    "#         Args:\n",
    "#             index_name (str): Name of the Pinecone index.\n",
    "#             dimension (int): Dimension of the embedding vectors (768 for stsb-xlm-r-multilingual).\n",
    "#             cloud (str): Cloud provider (e.g., \"aws\", \"gcp\", \"azure\").\n",
    "#             region (str): Region for the cloud provider (e.g., \"us-east-1\" for AWS).\n",
    "#         \"\"\"\n",
    "#         self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "#             chunk_size=700,\n",
    "#             chunk_overlap=150,\n",
    "#             length_function=len,\n",
    "#             separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "#         )\n",
    "#         self.embeddings = AdvancedEmbedder()\n",
    "#         self.index_name = index_name\n",
    "#         self.dimension = dimension\n",
    "#         self.cloud = cloud\n",
    "#         self.region = region\n",
    "#         self.pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "#         self.index = None\n",
    "#         self._initialize_pinecone()\n",
    "\n",
    "#     def _initialize_pinecone(self) -> None:\n",
    "#         \"\"\"Initialize or connect to a Pinecone index with region validation.\"\"\"\n",
    "#         supported_regions = {\n",
    "#             \"aws\": [\"us-east-1\", \"us-west-2\", \"eu-west-1\"],\n",
    "#             \"gcp\": [\"us-central1\", \"europe-west1\"],\n",
    "#             \"azure\": [\"eastus\"]\n",
    "#         }\n",
    "#         if self.cloud not in supported_regions:\n",
    "#             raise ValueError(f\"Unsupported cloud provider: {self.cloud}. Supported: {list(supported_regions.keys())}\")\n",
    "#         if self.region not in supported_regions[self.cloud]:\n",
    "#             raise ValueError(f\"Region {self.region} not supported for {self.cloud}. Supported: {supported_regions[self.cloud]}\")\n",
    "\n",
    "#         if self.index_name not in self.pc.list_indexes().names():\n",
    "#             try:\n",
    "#                 self.pc.create_index(\n",
    "#                     name=self.index_name,\n",
    "#                     dimension=self.dimension,\n",
    "#                     metric=\"cosine\",\n",
    "#                     spec=ServerlessSpec(cloud=self.cloud, region=self.region)\n",
    "#                 )\n",
    "#                 logger.info(f\"Created new Pinecone index: {self.index_name} with dimension {self.dimension}\")\n",
    "#                 time.sleep(5)  # Wait for index creation to propagate\n",
    "#             except Exception as e:\n",
    "#                 logger.error(f\"Failed to create index {self.index_name}: {str(e)}\")\n",
    "#                 raise\n",
    "#         else:\n",
    "#             logger.info(f\"Index {self.index_name} already exists.\")\n",
    "\n",
    "#         self.index = self.pc.Index(self.index_name)\n",
    "#         logger.info(f\"Connected to Pinecone index: {self.index_name}\")\n",
    "\n",
    "#     def _parallel_upsert(self, vectors: List[dict], namespace: str, batch_size=100) -> None:\n",
    "#         \"\"\"Perform parallel upsert for faster indexing with detailed logging.\"\"\"\n",
    "#         def upsert_batch(batch):\n",
    "#             start_time = time.time()\n",
    "#             self.index.upsert(vectors=batch, namespace=namespace)\n",
    "#             logger.info(\n",
    "#                 f\"Upserted batch of {len(batch)} vectors in {time.time() - start_time:.2f} seconds \"\n",
    "#                 f\"(namespace: {namespace}, IDs: {[v['id'] for v in batch]})\"\n",
    "#             )\n",
    "\n",
    "#         total_chunks = len(vectors)\n",
    "#         logger.info(f\"Starting parallel upsert of {total_chunks} vectors into namespace: {namespace}\")\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#             for i in range(0, len(vectors), batch_size):\n",
    "#                 batch = vectors[i:i + batch_size]\n",
    "#                 executor.submit(upsert_batch, batch)\n",
    "\n",
    "#         logger.info(f\"Completed upsert of {total_chunks} vectors in {time.time() - start_time:.2f} seconds\")\n",
    "#         time.sleep(2)  # Wait for upsert to propagate\n",
    "\n",
    "#     def load_document(self, file_path: str, namespace: str = \"default\") -> None:\n",
    "#         \"\"\"Load and process a document with advanced strategies and detailed logging.\"\"\"\n",
    "#         file_path = os.path.normpath(file_path.strip())\n",
    "#         if not os.path.isfile(file_path):\n",
    "#             raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "#         ext = os.path.splitext(file_path)[1].lower()\n",
    "#         loaders = {\n",
    "#             \".pdf\": PyPDFLoader,\n",
    "#             \".docx\": Docx2txtLoader,\n",
    "#             \".txt\": TextLoader,\n",
    "#             \".xml\": UnstructuredXMLLoader\n",
    "#         }\n",
    "#         if ext not in loaders:\n",
    "#             raise ValueError(f\"Unsupported file type: {ext}. Supported: {list(loaders.keys())}\")\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         loader = loaders[ext](file_path)\n",
    "#         docs = loader.load()\n",
    "#         logger.info(f\"Loaded document: {file_path} in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         chunks = self.text_splitter.split_documents(docs)\n",
    "#         chunk_texts = [doc.page_content for doc in chunks]\n",
    "#         logger.info(f\"Split document into {len(chunks)} chunks in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "#         embeddings = self.embeddings.embed_documents(chunk_texts)\n",
    "#         vectors = []\n",
    "#         start_time = time.time()\n",
    "#         for i, (text, embedding) in enumerate(zip(chunk_texts, embeddings)):\n",
    "#             vector_id = f\"{file_path}_{i}\"\n",
    "#             vectors.append({\"id\": vector_id, \"values\": embedding, \"metadata\": {\"text\": text}})\n",
    "#             if i < 3:\n",
    "#                 logger.info(\n",
    "#                     f\"Created vector {i+1}/{len(chunk_texts)}: ID={vector_id}, \"\n",
    "#                     f\"Embedding length={len(embedding)}, Sample text={text[:50]}...\"\n",
    "#                 )\n",
    "#         logger.info(f\"Generated {len(vectors)} vectors in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "#         self._parallel_upsert(vectors, namespace)\n",
    "#         logger.info(f\"Completed loading and upserting {file_path} into namespace: {namespace}\")\n",
    "\n",
    "#     def retrieve(self, query: str, k: int = 5, namespace: str = \"default\") -> List[str]:  #! improve this retriever  use advance techniques to retrieve\n",
    "#         \"\"\"Retrieve relevant document chunks.\"\"\"\n",
    "#         if not self.index:\n",
    "#             logger.warning(\"No Pinecone index initialized.\")\n",
    "#             return []\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         query_embedding = self.embeddings.embed_query(query)\n",
    "#         result = self.index.query(\n",
    "#             vector=query_embedding,\n",
    "#             top_k=k,\n",
    "#             include_metadata=True,\n",
    "#             namespace=namespace\n",
    "#         )\n",
    "#         retrieved_docs = [match[\"metadata\"][\"text\"] for match in result[\"matches\"]]\n",
    "#         logger.info(f\"Retrieved {len(retrieved_docs)} chunks for query in {time.time() - start_time:.2f} seconds\")\n",
    "#         return retrieved_docs\n",
    "\n",
    "#     def clear(self, namespace: str = \"default\") -> None:\n",
    "#         \"\"\"Clear a namespace in the Pinecone index.\"\"\"\n",
    "#         if self.index:\n",
    "#             start_time = time.time()\n",
    "#             self.index.delete(delete_all=True, namespace=namespace)\n",
    "#             logger.info(f\"Cleared Pinecone namespace: {namespace} in {time.time() - start_time:.2f} seconds\")\n",
    "#         else:\n",
    "#             logger.warning(\"No Pinecone index to clear.\")\n",
    "\n",
    "\n",
    "\n",
    "# # Optional: Initialize a main RAG system instance\n",
    "# rag = RAGSystem(index_name=\"rag-index\", dimension=768, cloud=\"aws\", region=\"us-east-1\")\n",
    "# print(\"RAG System with Pinecone initialized (768 dimensions).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a859a88d-b4b6-44d0-8efd-3007b660460b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c4291d-7102-491d-a116-c260365036b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d81f8-b55a-48bd-b7c1-63784e9ae573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d74b74e8-91bd-47a1-8a5d-b295920eeab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow nodes defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def intent_agent(state: ChatbotState) -> ChatbotState:\n",
    "    state['query'] = state['query'].strip()\n",
    "    if len(state['query']) < 5 or not any(c.isalpha() for c in state['query']):  # Check for short or non-alphabetic input\n",
    "        state['response'] = \"Sorry, your input is unclear. Could you please provide more details?\"\n",
    "        state['requires_human_review'] = False\n",
    "        logger.info(\"Ambiguous input detected, requesting clarification\")\n",
    "        return state\n",
    "    state['prompt_type'] = prompt_lib.route_prompt(state['query'])\n",
    "    state['conversation_history'].append({\"query\": state['query'], \"prompt_type\": state['prompt_type'], \"response\": \"\"})\n",
    "    logger.info(f\"Intent detected: {state['prompt_type']}\")\n",
    "    return state\n",
    "\n",
    "    \n",
    "# def retrieval_agent(state: ChatbotState) -> ChatbotState:\n",
    "#     state['retrieved_docs'] = rag.retrieve(state['query'])\n",
    "#     logger.info(f\"Retrieved {len(state['retrieved_docs'])} chunks\")\n",
    "#     return state   #! modify ths agent this agent should  modify the retrieve docus and provide the answer to the user query accordingly\n",
    "\n",
    "def retrieval_agent(state: ChatbotState) -> ChatbotState:\n",
    "    \"\"\"Retrieves and refines relevant document chunks based on the query.\"\"\"\n",
    "    # Step 1: Retrieve initial documents\n",
    "    initial_docs = rag.retrieve(state['query'])  # Increase to 5 for broader coverage\n",
    "    logger.info(f\"Initially retrieved {len(initial_docs)} chunks\")\n",
    "\n",
    "    if not initial_docs:\n",
    "        state['retrieved_docs'] = []\n",
    "        state['response'] = \"No relevant content found in the loaded document.\"\n",
    "        logger.info(\"No documents retrieved\")\n",
    "        return state\n",
    "\n",
    "    # Step 2: Use LLM to refine and filter retrieved chunks\n",
    "    context = \"\\n\\n\".join(initial_docs)\n",
    "    refinement_prompt = f\"\"\"Given the query: '{state['query']}', refine this context into a concise version:\n",
    "                            {context}\n",
    "                            - Keep only the parts directly relevant to the query.\n",
    "                            - Remove redundant or off-topic information.\n",
    "                            - Return the refined text or 'Insufficient relevant content' if nothing matches.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are an expert at refining text. Focus strictly on relevance to the query.\"),\n",
    "        HumanMessage(content=refinement_prompt)\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        refined_response = llm.invoke(messages).content.strip()\n",
    "        if \"insufficient relevant content\" in refined_response.lower():\n",
    "            state['retrieved_docs'] = []\n",
    "            state['response'] = \"The document doesn’t contain enough relevant information for your query.\"\n",
    "            logger.info(\"Refined retrieval: No relevant content found\")\n",
    "        else:\n",
    "            state['retrieved_docs'] = [refined_response]  # Store as a single refined chunk\n",
    "            logger.info(f\"Refined {len(initial_docs)} chunks into 1 query-specific chunk\")\n",
    "    except Exception as e:\n",
    "        state['retrieved_docs'] = initial_docs  # Fallback to unrefined docs\n",
    "        logger.error(f\"Retrieval refinement failed: {str(e)}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def qa_agent(state: ChatbotState) -> ChatbotState:\n",
    "    if not state['retrieved_docs']:\n",
    "        state['response'] = \"No document loaded or no relevant content found.\"\n",
    "    else:\n",
    "        context = \"\\n\".join(state['retrieved_docs'])\n",
    "        prompt = prompt_lib.get_prompt(\"qa\", state['query'], context)\n",
    "        messages = [\n",
    "            SystemMessage(content=\"Answer questions accurately using only the provided context. If the context lacks sufficient information, say so.\"),\n",
    "            HumanMessage(content=prompt.format(context=context, question=state['query']))\n",
    "        ]\n",
    "        state['response'] = llm.invoke(messages).content\n",
    "        if \"no relevant content\" in state['response'].lower():\n",
    "            state['response'] = \"The document doesn’t provide enough information to answer this question.\"\n",
    "    logger.info(\"QA response generated\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def summarization_agent(state: ChatbotState) -> ChatbotState:     #! this summarizarion agent should retrieve documnet and summarize the  content.based on retrieved content.\n",
    "#     if not state['retrieved_docs']:\n",
    "#         state['response'] = \"No document loaded or no relevant content found.\"\n",
    "#     else:\n",
    "#         context = \"\\n\".join(state['retrieved_docs'])\n",
    "#         prompt = prompt_lib.get_prompt(\"summarization\", state['query'], context)\n",
    "#         messages = [\n",
    "#             SystemMessage(content=\"You are a summarization expert. Summarize the provided context concisely.\"),\n",
    "#             HumanMessage(content=prompt.format(context=context))\n",
    "#         ]\n",
    "#         state['response'] = llm.invoke(messages).content\n",
    "#         if \"insufficient\" in state['response'].lower():\n",
    "#             state['response'] = \"The document doesn’t provide enough information for a complete summary.\"\n",
    "#     logger.info(\"Summarization response generated\")\n",
    "#     return state\n",
    "\n",
    "\n",
    "\n",
    "def guardrail_agent(state: ChatbotState) -> ChatbotState:\n",
    "    # Safety check\n",
    "    messages = [\n",
    "        SystemMessage(content=\"Evaluate this response: {response}. if everything looks good then only procced. . \"),\n",
    "        HumanMessage(content=state['response'])\n",
    "    ]\n",
    "    # check = llm.invoke(messages).content.strip().lower()\n",
    "    # state['requires_human_review']\n",
    "    # if state['requires_human_review']:\n",
    "    #     state['response'] = \"I'm sorry, I can't provide that response due to ethical guidelines.\"\n",
    "    #     logger.info(f\"Guardrail check: unsafe - response blocked\")\n",
    "    return state\n",
    "\n",
    "def summarization_agent(state: ChatbotState) -> ChatbotState:\n",
    "    \"\"\"Generates a query-specific summary based on retrieved document content.\"\"\"\n",
    "    if not state['retrieved_docs']:\n",
    "        state['response'] = \"No document loaded or no relevant content found to summarize.\"\n",
    "        logger.info(\"No content available for summarization\")\n",
    "        return state\n",
    "\n",
    "    # Use the refined context from retrieval_agent\n",
    "    context = \"\\n\".join(state['retrieved_docs'])\n",
    "    \n",
    "    # Tailor the summarization prompt based on the query\n",
    "    prompt = prompt_lib.get_prompt(\"summarization\", state['query'], context)\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a summarization expert. Summarize the provided context concisely, focusing on aspects relevant to the query.\"),\n",
    "        HumanMessage(content=prompt.format(context=context))\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        summary = llm.invoke(messages).content.strip()\n",
    "        if \"insufficient\" in summary.lower() or not summary:\n",
    "            state['response'] = \"The document doesn’t provide enough relevant information for a meaningful summary.\"\n",
    "        else:\n",
    "            # Add structure to the summary based on query intent\n",
    "            if \"experience\" in state['query'].lower() or \"projects\" in state['query'].lower():\n",
    "                state['response'] = f\"Summary of relevant experience/projects:\\n{summary}\"\n",
    "            else:\n",
    "                state['response'] = f\"Summary:\\n{summary}\"\n",
    "        logger.info(\"Summarization response generated\")\n",
    "    except Exception as e:\n",
    "        state['response'] = \"Sorry, I couldn’t summarize the content due to an issue.\"\n",
    "        state['requires_human_review'] = True\n",
    "        logger.error(f\"Summarization failed: {str(e)}\")\n",
    "    \n",
    "    return state\n",
    "    \n",
    "\n",
    "\n",
    "# def output_node(state: ChatbotState) -> ChatbotState:\n",
    "#     state['conversation_history'][-1][\"response\"] = state['response']\n",
    "#     state['metadata']['timestamp'] = datetime.now().isoformat()\n",
    "#     if state['requires_human_review']:\n",
    "#         state['response'] += \"\\n[Flagged for human review]\"\n",
    "#     logger.info(f\"Final response: {state['response']}\")\n",
    "#     return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Workflow nodes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9474d79-6c25-4032-8cdc-76b017bbe449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow built.\n"
     ]
    }
   ],
   "source": [
    "def build_workflow():\n",
    "    workflow = StateGraph(ChatbotState)\n",
    "    # Define nodes\n",
    "    workflow.add_node(\"intent\", intent_agent)\n",
    "    workflow.add_node(\"retrieval\", retrieval_agent)\n",
    "    workflow.add_node(\"summarization\", summarization_agent)\n",
    "    workflow.add_node(\"qa\", qa_agent)\n",
    "    workflow.add_node(\"guardrail\", guardrail_agent)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"intent\")\n",
    "    \n",
    "    # From intent to retrieval for all cases\n",
    "    workflow.add_edge(\"intent\", \"retrieval\")\n",
    "    \n",
    "    # Conditional routing after retrieval based on intent\n",
    "    workflow.add_conditional_edges(\n",
    "        \"retrieval\",\n",
    "        lambda state: state['prompt_type'],\n",
    "        {\n",
    "            \"qa\": \"qa\",\n",
    "            \"summarization\": \"summarization\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Edges to guardrail (last step before end)\n",
    "    workflow.add_edge(\"qa\", \"guardrail\")\n",
    "    workflow.add_edge(\"summarization\", \"guardrail\")\n",
    "    \n",
    "    # Guardrail to end\n",
    "    workflow.add_edge(\"guardrail\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "app = build_workflow()\n",
    "print(\"Workflow built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630dece-0d2b-48b6-9b41-e5af8a0beaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ac8796-255d-4cd6-a77b-8b16baff7097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 17:41:41,759 - INFO - Workflow initialized.\n",
      "C:\\Users\\Yash\\AppData\\Local\\Temp\\ipykernel_15128\\1876915756.py:136: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"shutdown\")\n",
      "2025-03-25 17:41:41,768 - INFO - Starting FastAPI server...\n",
      "WARNING:  You must pass the application as an import string to enable 'reload' or 'workers'.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yash\\AppData\\Local\\anaconda3\\envs\\env_ai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, File, UploadFile, HTTPException, Form\n",
    "from typing import Optional\n",
    "import os\n",
    "import logging\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "from langgraph.graph import END\n",
    "import aiofiles  # For async file operations\n",
    "import uvicorn\n",
    "\n",
    "# Import your main backend components (adjust as needed)\n",
    "# from your_workflow_module import build_workflow, ChatbotState, rag\n",
    "\n",
    "# Apply nest_asyncio for compatibility (only if needed in your environment)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',\n",
    "    handlers=[logging.FileHandler(\"fastapi.log\"), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI app with production settings\n",
    "app = FastAPI(\n",
    "    title=\"AI Knowledge Assistant API\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\" if os.getenv(\"ENV\") != \"production\" else None,  # Disable docs in production\n",
    "    redoc_url=None  # Disable ReDoc\n",
    ")\n",
    "\n",
    "# Build workflow at startup (assumed to be async-compatible)\n",
    "workflow_app = build_workflow()\n",
    "logger.info(\"Workflow initialized.\")\n",
    "\n",
    "# Constants\n",
    "MAX_FILE_SIZE = 1_048_576  # 1 MB\n",
    "TEMP_DIR = \"/tmp\"\n",
    "\n",
    "# Ensure temp directory exists\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "# API Endpoints\n",
    "@app.post(\"/query\")\n",
    "async def query_without_file(query: str = Form(...), prompt_version: Optional[str] = Form(\"1.1\")):\n",
    "    \"\"\"Handle text-only queries asynchronously.\"\"\"\n",
    "    try:\n",
    "        state = ChatbotState(\n",
    "            query=query,\n",
    "            retrieved_docs=[],\n",
    "            response=\"\",\n",
    "            metadata={\"status\": \"pending\", \"prompt_version\": prompt_version},\n",
    "            prompt_type=\"\",\n",
    "            conversation_history=[],\n",
    "            requires_human_review=False\n",
    "        )\n",
    "        logger.info(f\"Processing query: {query}\")\n",
    "        result = await workflow_app.ainvoke(state)  # Ensure ainvoke is async\n",
    "        logger.info(\"Query processed successfully.\")\n",
    "        return {\n",
    "            \"response\": result[\"response\"],\n",
    "            \"metadata\": result[\"metadata\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing query: {str(e)}\", exc_info=True)\n",
    "        raise HTTPException(status_code=500, detail=f\"Internal Server Error: {str(e)}\")\n",
    "\n",
    "@app.post(\"/query_with_file\")\n",
    "async def query_with_file(\n",
    "    query: str = Form(...),\n",
    "    file: UploadFile = File(...),\n",
    "    prompt_version: Optional[str] = Form(\"1.1\")\n",
    "):\n",
    "    \"\"\"Handle queries with file uploads asynchronously.\"\"\"\n",
    "    try:\n",
    "        # Read file content asynchronously\n",
    "        file_content = await file.read()\n",
    "        if len(file_content) > MAX_FILE_SIZE:\n",
    "            raise HTTPException(status_code=400, detail=\"File size exceeds 1 MB limit.\")\n",
    "\n",
    "        # Save uploaded file temporarily using async I/O\n",
    "        file_path = os.path.join(TEMP_DIR, file.filename)\n",
    "        async with aiofiles.open(file_path, \"wb\") as f:\n",
    "            await f.write(file_content)\n",
    "        \n",
    "        # Load document into RAG (assuming rag.load_document is async or can be awaited)\n",
    "        await asyncio.to_thread(rag.load_document, file_path)  # Wrap in thread if sync\n",
    "        logger.info(f\"Document loaded: {file.filename}\")\n",
    "\n",
    "        # If query is just to load the file, return confirmation\n",
    "        if query.lower().strip() in [\"load file\", \"upload file\", \"\"]:\n",
    "            return {\n",
    "                \"response\": f\"File '{file.filename}' loaded and ready for RAG.\",\n",
    "                \"metadata\": {\"status\": \"loaded\", \"prompt_version\": prompt_version, \"file\": file.filename}\n",
    "            }\n",
    "\n",
    "        # Process the query with the workflow\n",
    "        state = ChatbotState(\n",
    "            query=query,\n",
    "            retrieved_docs=[],\n",
    "            response=\"\",\n",
    "            metadata={\"status\": \"pending\", \"prompt_version\": prompt_version, \"file\": file.filename},\n",
    "            prompt_type=\"\",\n",
    "            conversation_history=[],\n",
    "            requires_human_review=False\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Processing query with file: {query}\")\n",
    "        result = await workflow_app.ainvoke(state)  # Ensure ainvoke is async\n",
    "        \n",
    "        return {\n",
    "            \"response\": result[\"response\"],\n",
    "            \"metadata\": result[\"metadata\"]\n",
    "        }\n",
    "    except HTTPException as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing query with file: {str(e)}\", exc_info=True)\n",
    "        raise HTTPException(status_code=500, detail=f\"Internal Server Error: {str(e)}\")\n",
    "    finally:\n",
    "        # Cleanup: Remove temp file asynchronously\n",
    "        if os.path.exists(file_path):\n",
    "            await asyncio.to_thread(os.remove, file_path)  # Wrap in thread if sync\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return {\"status\": \"healthy\", \"date\": \"2025-03-25\"}\n",
    "\n",
    "# Graceful shutdown\n",
    "@app.on_event(\"shutdown\")\n",
    "async def shutdown_event():\n",
    "    \"\"\"Cleanup on shutdown.\"\"\"\n",
    "    logger.info(\"Shutting down FastAPI server...\")\n",
    "    # Add any cleanup logic here (e.g., closing connections)\n",
    "\n",
    "# Run the app with production-grade settings\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting FastAPI server...\")\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        workers=int(os.getenv(\"UVICORN_WORKERS\", 4)),  # Adjust workers for production\n",
    "        log_level=\"info\",\n",
    "        timeout_keep_alive=1000  # Handle long-lived connections\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5638249-9575-4100-a50a-7fd3fbf4900b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a7a1a-8fb6-45a9-ad9d-4ec3e8d13624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1784e8-828f-47ff-b358-7193f8f4f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastapi import FastAPI, File, UploadFile, HTTPException, Form\n",
    "# from typing import Optional\n",
    "# import os\n",
    "# import logging\n",
    "# from dotenv import load_dotenv\n",
    "# import nest_asyncio\n",
    "# from langgraph.graph import END\n",
    "\n",
    "# # Import your main backend components\n",
    "# # from your_workflow_module import build_workflow, ChatbotState, rag  # Adjust this import based on your file structure\n",
    "\n",
    "# # Apply nest_asyncio for compatibility\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',\n",
    "#     handlers=[logging.FileHandler(\"fastapi.log\"), logging.StreamHandler()]\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # Initialize FastAPI app\n",
    "# app = FastAPI(title=\"AI Knowledge Assistant API\", version=\"1.0.0\")\n",
    "\n",
    "# # Build workflow at startup\n",
    "# workflow_app = build_workflow()\n",
    "# logger.info(\"Workflow initialized.\")\n",
    "\n",
    "# # API Endpoints\n",
    "# @app.post(\"/query\")\n",
    "# async def query_without_file(query: str = Form(...), prompt_version: Optional[str] = Form(\"1.1\")):\n",
    "#     \"\"\"Handle text-only queries.\"\"\"\n",
    "#     try:\n",
    "#         state = ChatbotState(\n",
    "#             query=query,\n",
    "#             retrieved_docs=[],\n",
    "#             response=\"\",\n",
    "#             metadata={\"status\": \"pending\", \"prompt_version\": prompt_version},\n",
    "#             prompt_type=\"\",\n",
    "#             conversation_history=[],\n",
    "#             requires_human_review=False\n",
    "#         )\n",
    "#         logger.info(f\"Processing query: {query}\")\n",
    "#         result = await workflow_app.ainvoke(state)\n",
    "#         logger.info(\"Query processed successfully.\")\n",
    "#         return {\n",
    "#             \"response\": result[\"response\"],\n",
    "#             \"metadata\": result[\"metadata\"]\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error processing query: {str(e)}\", exc_info=True)\n",
    "#         raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n",
    "\n",
    "# @app.post(\"/query_with_file\")\n",
    "# async def query_with_file(\n",
    "#     query: str = Form(...),\n",
    "#     file: UploadFile = File(...),\n",
    "#     prompt_version: Optional[str] = Form(\"1.1\")\n",
    "# ):\n",
    "#     \"\"\"Handle queries with file uploads and confirm file loading.\"\"\"\n",
    "#     try:\n",
    "#         # Check file size (1 MB = 1,048,576 bytes)\n",
    "#         file_content = await file.read()\n",
    "#         if len(file_content) > 1_048_576:\n",
    "#             raise HTTPException(status_code=400, detail=\"File size exceeds 1 MB limit.\")\n",
    "        \n",
    "#         # Save uploaded file temporarily\n",
    "#         file_path = f\"/tmp/{file.filename}\"\n",
    "#         os.makedirs(\"/tmp\", exist_ok=True)\n",
    "#         with open(file_path, \"wb\") as f:\n",
    "#             f.write(file_content)\n",
    "        \n",
    "#         # Load document into RAG\n",
    "#         rag.load_document(file_path)\n",
    "#         logger.info(f\"Document loaded: {file.filename}\")\n",
    "\n",
    "#         # If the query is just to load the file, return a confirmation\n",
    "#         if query.lower().strip() in [\"load file\", \"upload file\", \"\"]:\n",
    "#             return {\n",
    "#                 \"response\": f\"File '{file.filename}' loaded and ready for RAG.\",\n",
    "#                 \"metadata\": {\"status\": \"loaded\", \"prompt_version\": prompt_version, \"file\": file.filename}\n",
    "#             }\n",
    "\n",
    "#         # Otherwise, process the query with the workflow\n",
    "#         state = ChatbotState(\n",
    "#             query=query,\n",
    "#             retrieved_docs=[],\n",
    "#             response=\"\",\n",
    "#             metadata={\"status\": \"pending\", \"prompt_version\": prompt_version, \"file\": file.filename},\n",
    "#             prompt_type=\"\",\n",
    "#             conversation_history=[],\n",
    "#             requires_human_review=False\n",
    "#         )\n",
    "        \n",
    "#         logger.info(f\"Processing query with file: {query}\")\n",
    "#         result = await workflow_app.ainvoke(state)\n",
    "        \n",
    "#         return {\n",
    "#             \"response\": result[\"response\"],\n",
    "#             \"metadata\": result[\"metadata\"]\n",
    "#         }\n",
    "#     except HTTPException as e:\n",
    "#         raise e\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error processing query with file: {str(e)}\", exc_info=True)\n",
    "#         raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n",
    "#     finally:\n",
    "#         if os.path.exists(file_path):\n",
    "#             os.remove(file_path)\n",
    "#         # Do not clear RAG here to persist the loaded document across queries\n",
    "#         # rag.clear()  # Uncomment if you want to clear after each query\n",
    "\n",
    "# @app.get(\"/health\")\n",
    "# async def health_check():\n",
    "#     return {\"status\": \"healthy\", \"date\": \"2025-03-25\"}\n",
    "\n",
    "# # Run the app\n",
    "# if __name__ == \"__main__\":\n",
    "#     import uvicorn\n",
    "#     logger.info(\"Starting FastAPI server...\")\n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7c0f8-f7b1-4c2a-97c0-a63812284106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
